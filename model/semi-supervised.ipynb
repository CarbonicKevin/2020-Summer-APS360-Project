{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this idea went pretty much nowhere\n",
    "\n",
    "The model simply learned to generate a black image, as it was the closest way of getting the right results\n",
    "It explains why all the accuracies tested generated approx the same avg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lib.train_AI_lib import *\n",
    "from lib.models.SemiSup import *\n",
    "from lib.models.AutoEncoders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using auto encoder to semi-supervise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batchsize=16; epoch=15; lr=0.01\n",
    "enc = autEncE('autEncE'); enc.cuda()\n",
    "enc.load_state_dict(torch.load('saved/TrainingRuns/AutoEncTrainer/autEncE_b64_te15_lr0.001/model_epoch14'))\n",
    "\n",
    "net = semiSupB(name='semiSupB', enc=enc); net.cuda()\n",
    "trainLoader, valLoader, testLoader = loadData(batchsize=batchsize)\n",
    "_ = trainNet(net, [trainLoader, valLoader], batchsize, epoch, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using auto encoder generatively (prob distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batchsize=16; epoch=15; lr=0.005\n",
    "enc = autEncC('autEncC'); enc.cuda()\n",
    "enc.load_state_dict(torch.load('saved/TrainingRuns/AutoEncTrainer/autEncC_b64_te15_lr0.001/model_epoch14'))\n",
    "\n",
    "net = genSemiA(name='genSemiA_encC', enc=enc); net.cuda()\n",
    "trainLoader, valLoader, testLoader = loadData(batchsize=batchsize)\n",
    "_ = trainNet(net, [trainLoader, valLoader], batchsize, epoch, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batchsize=16; epoch=15; lr=0.005\n",
    "enc = autEncD('autEncD'); enc.cuda()\n",
    "enc.load_state_dict(torch.load('saved/TrainingRuns/AutoEncTrainer/autEncD_b64_te15_lr0.001/model_epoch14'))\n",
    "\n",
    "net = genSemiA(name='genSemiA_encD', enc=enc); net.cuda()\n",
    "trainLoader, valLoader, testLoader = loadData(batchsize=batchsize)\n",
    "_ = trainNet(net, [trainLoader, valLoader], batchsize, epoch, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=16; epoch=15; lr=0.005\n",
    "enc = autEncE('autEncE'); enc.cuda()\n",
    "enc.load_state_dict(torch.load('saved/TrainingRuns/AutoEncTrainer/autEncE_b64_te15_lr0.001/model_epoch14'))\n",
    "\n",
    "net = genSemiA(name='genSemiA_encD', enc=enc); net.cuda()\n",
    "trainLoader, valLoader, testLoader = loadData(batchsize=batchsize)\n",
    "_ = trainNet(net, [trainLoader, valLoader], batchsize, epoch, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the auto encoder generatively (full)\n",
    "Since the above attempt at an autoencoder feeds in what is essentially a prob distribution into the CNN, attempting a different approach wherein i use the prediction to generate a mask to apply to the original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate the images to use (commented out unless required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enc = autEncC('autEncC'); enc.cuda()\n",
    "enc.load_state_dict(torch.load('saved/TrainingRuns/AutoEncTrainer/autEncC_b64_te15_lr0.001/model_epoch14'))\n",
    "genMaskedImg(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enc = autEncD('autEncD'); enc.cuda()\n",
    "enc.load_state_dict(torch.load('saved/TrainingRuns/AutoEncTrainer/autEncD_b64_te15_lr0.001/model_epoch14'))\n",
    "genMaskedImg(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enc = autEncE('autEncE'); enc.cuda()\n",
    "enc.load_state_dict(torch.load('saved/TrainingRuns/AutoEncTrainer/autEncE_b64_te15_lr0.001/model_epoch14'))\n",
    "genMaskedImg(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "batchsize=16; epoch=15; lr=0.001\n",
    "net = fullGenSemiA(name='fullGenSemiA_encC'); net.cuda()\n",
    "trainLoader, valLoader, testLoader = loadData(batchsize=batchsize, inPath=\"saved/autEncMasked/autEncC\")\n",
    "_ = trainNet(net, [trainLoader, valLoader], batchsize, epoch, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "batchsize=16; epoch=15; lr=0.001\n",
    "net = fullGenSemiA(name='fullGenSemiA_encD'); net.cuda()\n",
    "trainLoader, valLoader, testLoader = loadData(batchsize=batchsize, inPath=\"saved/autEncMasked/autEncD\")\n",
    "_ = trainNet(net, [trainLoader, valLoader], batchsize, epoch, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three models except encoder E showed promise, All with finial training errors below the baseline. Am begining to wonder, if the lables we have are 'categorized' into the discrete numbers of bounding boxes,\n",
    "can we round the output of the network then use the rounded output as the input into the loss function? Could the loss function then stay MSE, or could it work as Cross Entropy (most likely a classification function without functions like softmax, but still)\n",
    "\n",
    "Its also noted that the training loops are highly erratic, at once going down to 16, but then going to as high as 20. it's odd.\n",
    "\n",
    "Note 2: turns out the erratic behavour observed above was due to my evalutaion code, recoded evalutaion doedsn't produce such erratic outputs\n",
    "Retraining all networks to see which one works the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting to see if i can make this into a sudo-classification problem as defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretising the prediction into the loss function had no effect, further research will need to be conducted if there are loss fucntions fit for this task\n",
    "For now, the definition of the normal regression adam trainer will be trained to give this definition of accuracy, as it's a different metric than loss."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595467820819",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
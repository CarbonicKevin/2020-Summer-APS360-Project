{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this idea went pretty much nowhere\n",
    "\n",
    "The model simply learned to generate a black image, as it was the closest way of getting the right results\n",
    "It explains why all the accuracies tested generated approx the same avg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lib.train_AI_lib import *\n",
    "from lib.SemiSup import *\n",
    "from lib.AutoEncoders import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using auto encoder to semi-supervise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "batchsize=16; epoch=15; lr=0.01\n",
    "enc = autEncE('autEncE'); enc.cuda()\n",
    "enc.load_state_dict(torch.load('saved/TrainingRuns/AutoEncTrainer/autEncE_b64_te15_lr0.001/model_epoch14'))\n",
    "\n",
    "net = semiSupB('semiSupB', enc); net.cuda()\n",
    "trainLoader, valLoader, testLoader = loadData(batchsize=batchsize, preCalc=0)\n",
    "_ = trainNet(net, [trainLoader, valLoader], batchsize, epoch, lr)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely Terrible performance, either gets stuck at ~21 rsme or dosen't converge at all. I assume the reason is because the aut. enc is not symm, so the\n",
    "convolution in the later states (and the data that it extracts based on the code) is not trivial or easy to replicate\n",
    "\n",
    "Attempting to use the auto encoder generatively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using auto encoder generatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batchsize=16; epoch=15; lr=0.005\n",
    "enc = autEncC('autEncC'); enc.cuda()\n",
    "enc.load_state_dict(torch.load('saved/TrainingRuns/AutoEncTrainer/autEncC_b64_te15_lr0.001/model_epoch14'))\n",
    "\n",
    "net = genSemiA('genSemiA_encC', enc); net.cuda()\n",
    "trainLoader, valLoader, testLoader = loadData(batchsize=batchsize, preCalc=0)\n",
    "_ = trainNet(net, [trainLoader, valLoader], batchsize, epoch, lr, trainType='RegAdam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowest Training   ~15 rmse\n",
    "Lowest Validation ~16 rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batchsize=16; epoch=15; lr=0.005\n",
    "enc = autEncD('autEncD'); enc.cuda()\n",
    "enc.load_state_dict(torch.load('saved/TrainingRuns/AutoEncTrainer/autEncD_b64_te15_lr0.001/model_epoch14'))\n",
    "\n",
    "net = genSemiA('genSemiA_encD', enc); net.cuda()\n",
    "trainLoader, valLoader, testLoader = loadData(batchsize=batchsize, preCalc=0)\n",
    "_ = trainNet(net, [trainLoader, valLoader], batchsize, epoch, lr, trainType='RegAdam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowest Training   ~11 rmse\n",
    "Lowest Validation ~15 rmse\n",
    "\n",
    "Overfitting Observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three models except encoder E showed promise, All with finial training errors below the baseline. Am begining to wonder, if the lables we have are 'categorized' into the discrete numbers of bounding boxes,\n",
    "can we round the output of the network then use the rounded output as the input into the loss function? Could the loss function then stay MSE, or could it work as Cross Entropy (most likely a classification function without functions like softmax, but still)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting to see if i can make this into a sudo-classification problem as defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "batchsize=16; epoch=15; lr=0.005\n",
    "enc = autEncD('autEncD'); enc.cuda()\n",
    "enc.load_state_dict(torch.load('saved/TrainingRuns/AutoEncTrainer/autEncD_b64_te15_lr0.001/model_epoch14'))\n",
    "\n",
    "net = genSemiA('genDiscA_encD', enc); net.cuda()\n",
    "trainLoader, valLoader, testLoader = loadData(batchsize=batchsize, preCalc=0)\n",
    "_ = trainNet(net, [trainLoader, valLoader], batchsize, epoch, lr, trainType='discRegAdam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretising the prediction into the loss function had no effect, further research will need to be conducted if there are loss fucntions fit for this task\n",
    "For now, the definition of the normal regression adam trainer will be trained to give this definition of accuracy, as it's a different metric than loss."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595000917445",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "APS360 Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rJy1gVvlpDT",
        "colab_type": "text"
      },
      "source": [
        "# Project Model v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSLMJtDOlsba",
        "colab_type": "text"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJJirJVVovrS",
        "colab_type": "text"
      },
      "source": [
        "###Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSkfeoFml-vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as utilData\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import ast\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMXPb4ykoEnJ",
        "colab_type": "text"
      },
      "source": [
        "###Fcn: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQmN3x5WmF4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def splitData(ratio=[0.8, 0.1, 0.1], iPath=\"../data/working-wheat-data/train\", oPath=\"../saved/splitData\", koPath=\"../data/working-wheat-data/train.csv\"):\n",
        "    \"\"\"\n",
        "    Function that takes the given input path (iPath) splits the image set into a given ratio then saves the names of images in each list to files\n",
        "    output to output path (oPath)\n",
        "\n",
        "    Arguments:\n",
        "        ratio: a len-3 list containing the ratios of training, validation and testing data. default = [0.8, 0.1, 0.1]\n",
        "        iPath: input path. default = \"../data/working-wheat-data/train\"\n",
        "        oPath: output path. default = \"../saved/splitData\"\n",
        "        koPath: known bbox output csv path\n",
        "    Returns:\n",
        "        boolean of 0 or 1. fail or success.\n",
        "    Creates Files:\n",
        "        trainData\n",
        "        valData\n",
        "        testData\n",
        "\n",
        "        files are saved via torch.save. each contains a dictionary.\n",
        "        Keys of dictionaries are file names\n",
        "        Values of dictionaries are a list of bboxes in the form [[bbox1], [bbox2], ...]\n",
        "    \"\"\"\n",
        "\n",
        "    if sum(ratio) != 1:\n",
        "        print(\"<splitData> error in ratio: does not sum to 1\")\n",
        "    if len(ratio) != 3:\n",
        "        print(\"<splitData> error in ratio: input must be of length 3\")\n",
        "\n",
        "    try: os.makedirs(oPath)  # Making directory w/ error check\n",
        "    except FileExistsError: None\n",
        "    except: print(\"<splitData> error creating folder {}\".format(oPath)); return(0)\n",
        "    else: None\n",
        "\n",
        "    np.random.seed(1234)  # Shuffling the data\n",
        "    files = [f for f in os.listdir(iPath) if os.path.isfile(os.path.join(iPath, f))]\n",
        "    np.random.shuffle(files)\n",
        "\n",
        "    # Splitting images\n",
        "    tLen = len(files); trainIndex=round(tLen*ratio[0])\n",
        "    trainList=files[:trainIndex]; remain=files[trainIndex:]\n",
        "    rLen = len(remain); valIndex=round(rLen*(ratio[1]/(ratio[1]+ratio[2])))\n",
        "    valList=remain[:valIndex]; testList=remain[valIndex:]\n",
        "\n",
        "    # append .csv info into dictionary\n",
        "    trainDict = appendKnownOutputs(trainList, koPath)\n",
        "    valDict   = appendKnownOutputs(valList  , koPath)\n",
        "    testDict  = appendKnownOutputs(testList , koPath)\n",
        "\n",
        "    # Save dictionary\n",
        "    torch.save(trainDict, oPath+\"/trainData\")\n",
        "    torch.save(valDict  , oPath+\"/valData\"  )\n",
        "    torch.save(testDict , oPath+\"/testData\" )\n",
        "\n",
        "    return(1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuFCNt55oIwE",
        "colab_type": "text"
      },
      "source": [
        "###Fcn: Append Known Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaVf0RpcmOUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def appendKnownOutputs(imgList, koPath):\n",
        "    \"\"\"\n",
        "    helper function that takes a list of image file names, finds them in the train.csv file, and then creates a dictionary based on the results\n",
        "    key of dictionary is the file name, value of dictionary is a list of bbox lists [[bbox1], [bbox2], ...]\n",
        "    \n",
        "    Arguments:\n",
        "        imgList: List of image files names\n",
        "        koPath : Path to the CSV file\n",
        "\n",
        "    Returns:\n",
        "        the dictionary as described above\n",
        "    \"\"\"\n",
        "    imgDict = {}  # predefine dictionary\n",
        "    db = pd.read_csv(koPath, header=0)  # Open csv\n",
        "    for img in imgList:\n",
        "        imgN = img.split(\".jpg\")[0]\n",
        "        mask = db['image_id'].isin([imgN])  # Create a mask for the specific image name\n",
        "        relRow = db.loc[mask]  # Find entries only with the specific image name\n",
        "        imgDict[img] = [ast.literal_eval(bbox) for bbox in relRow['bbox']]  # Save all bboxes to dictionary\n",
        "    return(imgDict)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTv4yOdJL1Mg",
        "colab_type": "text"
      },
      "source": [
        "###Fcn: Prev Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf4sLq5GLyHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prevImages(dataPath=\"../saved/splitData/trainData\", imgFolder=\"../data/working-wheat-data/train\"):\n",
        "    \"\"\"\n",
        "    Function to simply test images and the bboxes\n",
        "    Arguments:\n",
        "        dataPath: path to csv file\n",
        "        imgFolder: path to images\n",
        "    \"\"\"\n",
        "    imgDict = torch.load(dataPath)  # Load Dictionary as generated by splitData\n",
        "    for i, (name, bboxs) in enumerate(imgDict.items()):\n",
        "        img = cv2.imread(imgFolder+\"/\"+name)  # read from image folder the image requested\n",
        "        # Add bboxes\n",
        "        [cv2.rectangle(img,(int(bbox[0]), int(bbox[1])),(int(bbox[0]+bbox[2]), int(bbox[1]+bbox[3])),(0,0,255),3) for bbox in bboxs]\n",
        "        cv2.imshow('image', img)  # Show bboxes\n",
        "        cv2.waitKey(0)  # wait for key press before moving to next image\n",
        "        if i > 20: break  # Break after 20 images"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV6MpUC4oH6B",
        "colab_type": "text"
      },
      "source": [
        "###Fcn: OpenCV Image Convert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q46QkKMnmfC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def openCVImgConvert(func, oPath, iPath=\"../data/working-wheat-data/train\"):\n",
        "    \"\"\"\n",
        "    Funtion to help quickly apply an openCV image transformation and save the outputs\n",
        "    Examples of Open CV features:\n",
        "    https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html#canny\n",
        "    https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_gradients/py_gradients.html#gradients\n",
        "    https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html#morphological-ops\n",
        "    https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html#filtering\n",
        "    Arguments:\n",
        "        func: a function which takes in an image, transforms it, and then returns the np array\n",
        "        oPath: the folder to which the new images are saved to\n",
        "        iPath: the folder to the original images\n",
        "    \"\"\"\n",
        "\n",
        "    files = [f for f in os.listdir(iPath) if os.path.isfile(os.path.join(iPath, f))]  # Creating a list of files in iPathDirectory\n",
        "    try: os.makedirs(oPath)  # Make the requested oPath\n",
        "    except FileExistsError: None\n",
        "    except: print(\"<openCVImgConvert> error creating folder {}\".format(oPath)); return(0)\n",
        "    else: None\n",
        "    \n",
        "    for i, f in enumerate(files):  # apply the given func() to every image in file\n",
        "        cv2.imwrite(oPath+\"/\"+f, func(cv2.imread(iPath+\"/\"+f)))\n",
        "        if i%200==0: print(\"Converted {:.2f}% of images\".format(100*i/len(files)))\n",
        "    print(\"Finished Conversion of Images\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoElZ702oRDU",
        "colab_type": "text"
      },
      "source": [
        "###Class: Image Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGR2LbY1mpY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class imgLoader(utilData.Dataset):\n",
        "    \"\"\"\n",
        "    Custom pytorch dataset for loading images\n",
        "    __init__:\n",
        "        Arguments:\n",
        "            dataPath: path to dictionary created by splitData()\n",
        "            imgPath : path to an image folder; note, as the class functions by looking up the file name, different folder directories can be given\n",
        "                      with the same dataPath as long as the images in those folders have a the same name as the original folder.\n",
        "    __len__ :\n",
        "        Function which is called when one uses the len() function on an imgLoader Object, returns the number of images\n",
        "    __getitem__(self, idx):\n",
        "        Function as required by a Map-style dataset. https://pytorch.org/docs/stable/data.html#map-style-datasets\n",
        "        returns:\n",
        "            img: A tensor version of the image\n",
        "            noBbox: The number of bboxes for this given image\n",
        "            imgName: name of image (for debug purposes)\n",
        "            bboxList: list of bounding boxes as defined as [[bbox1], [bbox2], ...] (for debug purposes)\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dataPath, imgPath):  # Defining inital variables\n",
        "        self.imgDict = torch.load(dataPath)\n",
        "        self.imgPath = imgPath + \"/\"\n",
        "        self.keyList = list(self.imgDict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return(len(self.imgDict))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        imgName = self.keyList[idx]  # grab the imageName of the given idx\n",
        "        img = cv2.imread(self.imgPath+imgName)  # load the image from file\n",
        "        # transform the file into a tensor, convert into a float\n",
        "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "        img = transform(img).float()\n",
        "\n",
        "        # return FOUR things, as described above.\n",
        "        return(img,  float(len(self.imgDict[imgName])), imgName, self.imgDict[imgName])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5-Juz2XMOCg",
        "colab_type": "text"
      },
      "source": [
        "###Fcn: Draw Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i06B1fknMNBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drawResults(modelpath, iters, trainLosses, valLosses, trainAcc, valAcc):\n",
        "    \"\"\"\n",
        "    Function used to quickly graph the results of training\n",
        "    Arguments:\n",
        "        modelpath             : path to save the image to\n",
        "        iters                 : list of epoch numbers\n",
        "        trainLosses, valLosses: lists of calculated losses per epoch\n",
        "        trainAcc, valAcc      : lists of calculated accuracies per epoch\n",
        "    \"\"\"\n",
        "    plt.plot(iters, trainAcc, '.-', label =  \"Training\")\n",
        "    plt.plot(iters,   valAcc, '.-', label = \"Validation\")\n",
        "    plt.title(\"Model Accuracy against Epoch No\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(); plt.grid()\n",
        "    plt.savefig(modelpath+\"Accuracy Graph.png\")\n",
        "    plt.show()\n",
        "    plt.cla()\n",
        "\n",
        "    plt.plot(iters, trainLosses, '.-', label =   \"Training\")\n",
        "    plt.plot(iters,   valLosses, '.-', label = \"Validation\")\n",
        "    plt.title(\"Model Loss against Epoch No\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "    plt.legend(); plt.grid()\n",
        "    plt.savefig(modelpath+\"Loss Graph.png\")\n",
        "    plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K_pYOSbMe6g",
        "colab_type": "text"
      },
      "source": [
        "###Fcn: Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqych5ROMZdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadData(batchsize, dictPath = \"../saved/splitData\", inPath = \"../data/working-wheat-data/train\"):\n",
        "    \"\"\"\n",
        "    Function to quickly batch generate a DataLoader\n",
        "    Arguments:\n",
        "        batchsize: requested batchsize\n",
        "        dataPath : path to dictionary created by splitData()\n",
        "        inPath   : path to an image folder; note, as the class functions by looking up the file name, different folder directories can be given\n",
        "                      with the same dataPath as long as the images in those folders have a the same name as the original folder.\n",
        "    Returns:\n",
        "        trainLoader, valLoder, testLoader: The DataLoaders batched as reqested\n",
        "    \"\"\"\n",
        "    trainData = imgLoader(dictPath+\"/trainData\", inPath)\n",
        "    valData   = imgLoader(dictPath+\"/valData\"  , inPath)\n",
        "    testData  = imgLoader(dictPath+\"/testData\" , inPath)\n",
        "\n",
        "    trainLoader = utilData.DataLoader(trainData, batch_size=batchsize, shuffle=1)\n",
        "    valLoader   = utilData.DataLoader(valData  , batch_size=batchsize, shuffle=1)\n",
        "    testLoader  = utilData.DataLoader(testData , batch_size=batchsize, shuffle=1)\n",
        "\n",
        "    return(trainLoader, valLoader, testLoader)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niccnL83MhMf",
        "colab_type": "text"
      },
      "source": [
        "###Fcn: Evaluate Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeRIQWAbMdkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evalRegress(net, loader, criterion, optimizer, isTraining, gpu=1):\n",
        "    \"\"\"\n",
        "    Function used in trainNet() to evaluate a given net for one epoch\n",
        "    Arguments:\n",
        "        net       : The net object\n",
        "        loader    : the loader whose images are being put through the network for evaluation\n",
        "        criterion : the criterion function\n",
        "        optimizer : the optimizer function\n",
        "        isTraining: Boolean to indicate if training should occur with evaluation, if True, optimizer will perform step\n",
        "        gpu       : Boolean to indicate if cuda is to be utilized\n",
        "    Returns:\n",
        "        Accuracy: The calculated accuracy over the epoch\n",
        "        avgLoss : The calculated average loss over the entire epoch\n",
        "    \"\"\"\n",
        "    lossTot = 0\n",
        "    for img, noBbox, _, _ in loader:  # if isTraining, computing loss and training, if not, then computing loss\n",
        "        if gpu and torch.cuda.is_available(): img = img.cuda(); noBbox = noBbox.cuda(); noBbox = noBbox.float()\n",
        "        pred = net(img); pred=torch.squeeze(pred, 1)\n",
        "        loss = criterion(pred, noBbox); lossTot += loss\n",
        "        if isTraining:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "    acc = []\n",
        "    for img, noBbox, _, _ in loader:  # Computing accuracy\n",
        "        if gpu and torch.cuda.is_available: img = img.cuda(); noBbox = noBbox.cuda()\n",
        "        pred = net(img)\n",
        "        acc += [torch.sum((pred-noBbox)**2)]\n",
        "\n",
        "    accuracy = sum(acc)/len(loader)\n",
        "    avgLoss = lossTot/len(loader)\n",
        "    return(accuracy, avgLoss)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3dnkecHMrMu",
        "colab_type": "text"
      },
      "source": [
        "###Fcn: Train Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Y__QSFMse4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainNet(net, data, batchsize, epochNo, lr, oPath=\"../saved\", trainType='RegAdam', evaluate=evalRegress, isCuda=1, draw=1):\n",
        "    \"\"\"\n",
        "    Big boy function that actually brings all of the function above together and actually trains the model\n",
        "    Arguments:\n",
        "        net      : the neural net object\n",
        "        data     : a 2 list of data loaders; [trainLoader, valLoader]\n",
        "        batchsize: the chosen batchsize\n",
        "        epochNo  : the chosen max epochNo\n",
        "        lr       : the chosen learning rate\n",
        "        oPath    : root output path for all files. if '/root' is given, will save to '/root/TrainingRuns/<Folder>/\n",
        "        trainType: string used to easily choose particular parameters such as criterion or optimizer\n",
        "        evaluate : name of the evaluation function, default to evalRegress\n",
        "        isCuda   : boolean to indicate if cuda should be used\n",
        "        draw     : boolean to indicate if the graph should be drawn\n",
        "    Returns:\n",
        "        iters, trainLosses, valLosses, trainAcc, valAcc: for debug purposes. All lists of the values at each epoch\n",
        "    \"\"\"\n",
        "    # Defining a saving path for ease of use\n",
        "    if trainType == 'RegAdam':\n",
        "        # Define criterion and optimizers\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "        functionName = \"RegAdamTrainer\"  # Name of the function used (incase we decide to use different optimizers, use alexnet etc)\n",
        "\n",
        "    modelpath = oPath+\"/TrainingRuns/{}/{}_b{}_te{}_lr{}/\".format(functionName, net.name, batchsize, epochNo, lr)\n",
        "    torch.manual_seed(1000)\n",
        "    try: os.makedirs(modelpath)  # Make the directory\n",
        "    except FileExistsError: None\n",
        "    except: print(\"Error Creating File\"); return()\n",
        "    else: None\n",
        "\n",
        "    trainData, valData = data[0], data[1]  # Loading Required Data\n",
        "\n",
        "    iters, trainLosses, valLosses, trainAcc, valAcc = [], [], [], [], []  # variables to graph and save\n",
        "    for epoch in range(epochNo):\n",
        "        if isCuda and torch.cuda.is_available():\n",
        "            start = torch.cuda.Event(enable_timing=True)\n",
        "            end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        if isCuda and torch.cuda.is_available(): start.record()\n",
        "        iters += [epoch]\n",
        "        #evaluate(net=net, loader=trainData, criterion=criterion, optimizer=optimizer, isTraining=True)\n",
        "        trainResults = evaluate(net=net, loader=trainData, criterion=criterion, optimizer=optimizer, isTraining=True)  # Calculating training error and loss\n",
        "        valResults   = evaluate(net=net, loader=  valData, criterion=criterion, optimizer=optimizer, isTraining=False)\n",
        "\n",
        "        if isCuda and torch.cuda.is_available(): end.record; torch.cuda.synchronize()\n",
        "\n",
        "        trainLosses += [trainResults[0]]; trainAcc += [trainResults[1]]  # Appending results\n",
        "        valLosses   += [  valResults[0]]; valAcc   += [  valResults[1]]\n",
        "        torch.save(net.state_dict(), modelpath+\"model_epoch{}\".format(epoch))\n",
        "\n",
        "        print(\"Epoch {} | Time Taken: {:.2f}s | Train acc: {:.10f},\\\n",
        "                Train loss: {:.10f} | Validation acc: {:.10f}, Validation loss: {:.10f}\\\n",
        "                \".format(epoch, start.elapsed_time(end)*0.001, trainAcc[epoch], trainLosses[epoch], valAcc[epoch], valLosses[epoch]))\n",
        "\n",
        "    if draw: drawResults(modelpath, iters, trainLosses, valLosses, trainAcc, valAcc)\n",
        "    return(iters, trainLosses, valLosses, trainAcc, valAcc)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSIOn1HCoef8",
        "colab_type": "text"
      },
      "source": [
        "###Sample code to run training module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w6QpyftM6wU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "88023025-9a37-4646-973a-2e66afd9d203"
      },
      "source": [
        "\"\"\" Eg (1)\n",
        "### Use example for imageLoader\n",
        "\n",
        "dictPath = \"../saved/splitData/trainData\"\n",
        "inPath   = \"../data/working-wheat-data/train\"\n",
        "\n",
        "loader   = imgLoader(dictPath, inPath)\n",
        "for imgName, img, output in loader:\n",
        "    ...\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" Eg (2)\n",
        "### Use example for openCVImgConvert\n",
        "\n",
        "outPath = \"../data/working-wheat-data/cv2_Canny_100_200\"\n",
        "inPath  = \"../data/working-wheat-data/train\"\n",
        "\n",
        "edgeDetect = lambda oImg: cv2.Canny(oImg, 100, 200)  # The\n",
        "openCVImgConvert(edgeDetect, outPath, inPath)  # Note: the images that are outputted have the same name as the original images, they are just in a different folder\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" Eg (3)\n",
        "### Use example for imageLoader, with the feature-detected images produced from Eg (2)\n",
        "\n",
        "dictPath = \"../saved/splitData/trainData\"  # NOTE: DICT PATH IS THE SAME AS WAS IN Eg (1). IT DOES NOT NEED TO CHANGE\n",
        "inPath   = \"../data/working-wheat-data/cv2_Canny_100_200\"  # ONLY THE IMAGE DIRECTORY HAS CHANGED\n",
        "\n",
        "loader   = imgLoader(dictPath, inPath)\n",
        "for imgName, img, output in loader:\n",
        "    ...\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" Eg (4)\n",
        "### Use example of prevImages with converted images from Eg (2)\n",
        "dictPath = \"../saved/splitData/trainData\"  # NOTE: DICT PATH IS THE SAME AS WAS IN Eg (1). IT DOES NOT NEED TO CHANGE\n",
        "inPath   = \"../data/working-wheat-data/cv2_Canny_100_200\"  # ONLY THE IMAGE DIRECTORY HAS CHANGED\n",
        "\n",
        "prevImages(dataPath=dictPath, imgFolder=inPath)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# BASE CODE FOR a neural net module to push into trainNet class\n",
        "class exNetClass(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super(exNetClass, self).__init__()\n",
        "        self.name = name\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3,   15,  6, stride=2)  # n = 1024 -> 510\n",
        "        self.conv2 = nn.Conv2d(15,  30,  6, stride=2)  # n = 510  -> 255\n",
        "        self.pool1 = nn.MaxPool2d(3, 2)                # n = 255  -> 127\n",
        "        self.conv3 = nn.Conv2d(30,  60,  6, stride=2)  # n = 127  -> 62\n",
        "        self.pool2 = nn.MaxPool2d(4, 2)                # n = 62   -> 30\n",
        "\n",
        "        self.fc1   = nn.Linear(29*29*60, 20)\n",
        "        self.fc2   = nn.Linear(20, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 29*29*60)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return(x)\n",
        "\n",
        "\n",
        "batchsize=64; lr=0.001; epochNo=10\n",
        "trainLoader, valLoader, _ = loadData(batchsize)\n",
        "netA = exNetClass(\"netA\"); netA.cuda()\n",
        "netATrain = trainNet(netA, [trainLoader, valLoader], batchsize, epochNo, lr)\n",
        "netATrain.train()\n",
        "\"\"\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# BASE CODE FOR a neural net module to push into trainNet class\\nclass exNetClass(nn.Module):\\n    def __init__(self, name):\\n        super(exNetClass, self).__init__()\\n        self.name = name\\n\\n        self.conv1 = nn.Conv2d(3,   15,  6, stride=2)  # n = 1024 -> 510\\n        self.conv2 = nn.Conv2d(15,  30,  6, stride=2)  # n = 510  -> 255\\n        self.pool1 = nn.MaxPool2d(3, 2)                # n = 255  -> 127\\n        self.conv3 = nn.Conv2d(30,  60,  6, stride=2)  # n = 127  -> 62\\n        self.pool2 = nn.MaxPool2d(4, 2)                # n = 62   -> 30\\n\\n        self.fc1   = nn.Linear(29*29*60, 20)\\n        self.fc2   = nn.Linear(20, 1)\\n\\n    def forward(self, x):\\n        x = self.pool1(F.relu(self.conv1(x)))\\n        x = F.relu(self.conv2(x))\\n        x = self.pool2(F.relu(self.conv3(x)))\\n        x = x.view(-1, 29*29*60)\\n        x = F.relu(self.fc1(x))\\n        x = self.fc2(x)\\n        return(x)\\n\\n\\nbatchsize=64; lr=0.001; epochNo=10\\ntrainLoader, valLoader, _ = loadData(batchsize)\\nnetA = exNetClass(\"netA\"); netA.cuda()\\nnetATrain = trainNet(netA, [trainLoader, valLoader], batchsize, epochNo, lr)\\nnetATrain.train()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-hpyewNltdd",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaBbN2i68aIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataPath = ''\n",
        "imgPath  = ''\n",
        "img_dataset = imgLoader(dataPath=dataPath, imgPath=imgPath)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}